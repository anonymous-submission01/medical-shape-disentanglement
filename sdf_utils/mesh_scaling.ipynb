{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import trimesh\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_global_min_max(file_list):\n",
        "    \"\"\"\n",
        "    Compute the global minimum and maximum vertex values across all shapes.\n",
        "    \"\"\"\n",
        "    global_min = np.inf\n",
        "    global_max = -np.inf\n",
        "    global_avg = 0\n",
        "\n",
        "    count = 0\n",
        "    for file in file_list:\n",
        "        mesh = trimesh.load(file)\n",
        "        vertices = mesh.vertices\n",
        "\n",
        "        # Update global min and max\n",
        "        global_min = min(global_min, vertices.min())\n",
        "        global_max = max(global_max, vertices.max())\n",
        "        global_avg += vertices.mean()\n",
        "\n",
        "        count += 1\n",
        "        if count % 50 == 0:\n",
        "            print(f\"Processed {count} shapes.\")\n",
        "\n",
        "    global_avg /= len(file_list)\n",
        "    \n",
        "    return global_min, global_max, global_avg\n",
        "\n",
        "def scale_mesh_to_uniform_range(mesh, global_min, global_max, target_min=-0.90, target_max=0.90):\n",
        "    \"\"\"\n",
        "    Scale a mesh such that the vertex coordinates are mapped to a global range [-0.95, 0.95],\n",
        "    while preserving the shape uniformly across all dimensions.\n",
        "    \"\"\"\n",
        "    # Compute the global range and the target range\n",
        "    global_range = global_max - global_min\n",
        "    target_range = target_max - target_min\n",
        "\n",
        "    # Compute the scaling factor based on the largest dimension range\n",
        "    scaling_factor = target_range / global_range\n",
        "\n",
        "    # Scale the vertices uniformly\n",
        "    vertices = mesh.vertices\n",
        "    scaled_vertices = (vertices - global_min) * scaling_factor + target_min\n",
        "    print(f\"Scaled vertices to range [{scaled_vertices.min()}, {scaled_vertices.max()}]\")\n",
        "\n",
        "    # Update the mesh vertices\n",
        "    mesh.vertices = scaled_vertices\n",
        "\n",
        "    return mesh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Input files\n",
        "input_files = glob.glob(\"[path to mesh folder: mesh_minimal_obj]\")  \n",
        "output_folder = \"[path to mesh folder: mesh_minimal_scaled_obj_files]\" \n",
        "\n",
        "target_min = -0.90\n",
        "target_max = 0.90\n",
        "\n",
        "# Create output folder if it doesn't exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute global min and max\n",
        "global_min, global_max, global_avg = compute_global_min_max(input_files)\n",
        "print(f\"Global min: {global_min}, Global max: {global_max}, Global avg: {global_avg}\")\n",
        "\n",
        "scaling_factor = (target_max - target_min) / (global_max - global_min)\n",
        "scale_info = {\n",
        "    \"global_min\": float(global_min),\n",
        "    \"global_max\": float(global_max),\n",
        "    \"target_min\": float(target_min),\n",
        "    \"target_max\": float(target_max),\n",
        "    \"scaling_factor\": float(scaling_factor),\n",
        "}\n",
        "scale_info_path = os.path.join(output_folder, \"scale_info.json\")\n",
        "with open(scale_info_path, \"w\") as f:\n",
        "    json.dump(scale_info, f, indent=2)\n",
        "print(f\"Scaling factor: {scaling_factor}\")\n",
        "print(f\"Saved scale info to: {scale_info_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process and save scaled meshes\n",
        "count = 0\n",
        "for input_file in input_files:\n",
        "    mesh = trimesh.load(input_file)\n",
        "    num_vertices = len(mesh.vertices)\n",
        "    num_faces = len(mesh.faces)\n",
        "    print(f\"{os.path.basename(input_file)}: vertices {num_vertices}, faces {num_faces}\")\n",
        "    scaled_mesh = scale_mesh_to_uniform_range(mesh, global_min, global_max, target_min, target_max)\n",
        "\n",
        "    # Save the scaled mesh\n",
        "    output_file = f\"{output_folder}{input_file.split('/')[-1]}\"\n",
        "    print(f\"Saving scaled mesh to: {output_file}\")\n",
        "    scaled_mesh.export(output_file)\n",
        "    count += 1\n",
        "    if count % 50 == 0:\n",
        "        print(f\"Processed {count} meshes.\")\n",
        "\n",
        "print(\"All meshes scaled and saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute global min and max\n",
        "input_files_scaled = glob.glob(\"[path to mesh folder: mesh_minimal_scaled_obj_files]\")  \n",
        "global_min, global_max, global_avg = compute_global_min_max(input_files_scaled)\n",
        "print(f\"Global min: {global_min}, Global max: {global_max}, Global avg: {global_avg}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Watertight meshes: 234/234\n"
          ]
        }
      ],
      "source": [
        "# Check watertightness for scaled meshes\n",
        "scaled_dir = '[path to mesh folder: mesh_minimal_scaled_obj_files]'\n",
        "mesh_paths = sorted(glob.glob(os.path.join(scaled_dir, '*.obj')))\n",
        "\n",
        "non_watertight = []\n",
        "for path in mesh_paths:\n",
        "    mesh = trimesh.load(path, force='mesh')\n",
        "    if not mesh.is_watertight:\n",
        "        non_watertight.append(path)\n",
        "\n",
        "print(f\"Watertight meshes: {len(mesh_paths) - len(non_watertight)}/{len(mesh_paths)}\")\n",
        "if non_watertight:\n",
        "    print('Non-watertight samples:')\n",
        "    for path in non_watertight[:10]:\n",
        "        print(path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Read the CSV file\n",
        "csv_path = \"[path to labels file]\"\n",
        "df = pd.read_csv(\n",
        "    csv_path,\n",
        "    names=[\"patient_id\", \"disease_diagnosis\", \"kl_grade\", \"gender\", \"age\"],\n",
        "    skiprows=1,\n",
        ")\n",
        "\n",
        "def normalize_patient_id(x):\n",
        "    if pd.isna(x):\n",
        "        return None\n",
        "    if isinstance(x, (int, float)) and float(x).is_integer():\n",
        "        return str(int(x))\n",
        "    s = str(x).strip()\n",
        "    try:\n",
        "        f = float(s)\n",
        "        if f.is_integer():\n",
        "            return str(int(f))\n",
        "    except ValueError:\n",
        "        pass\n",
        "    return s\n",
        "\n",
        "def to_num(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    if isinstance(x, str):\n",
        "        s = x.strip().lower()\n",
        "        if s in (\"m\", \"male\"):\n",
        "            return 0.0\n",
        "        if s in (\"f\", \"female\"):\n",
        "            return 1.0\n",
        "    return float(x)\n",
        "\n",
        "# Create labels dictionary\n",
        "labels = {}\n",
        "for _, row in df.iterrows():\n",
        "    patient_id = normalize_patient_id(row[\"patient_id\"])\n",
        "    if patient_id is None:\n",
        "        continue\n",
        "    label = np.array(\n",
        "        [\n",
        "            to_num(row[\"disease_diagnosis\"]),  # 0/1\n",
        "            to_num(row[\"kl_grade\"]),\n",
        "            to_num(row[\"gender\"]),\n",
        "            to_num(row[\"age\"]),\n",
        "        ],\n",
        "        dtype=np.float32,\n",
        "    )\n",
        "    labels[patient_id] = label\n",
        "\n",
        "print(f\"Created labels for {len(labels)} patients\")\n",
        "print(\"Sample labels:\")\n",
        "for i, (key, value) in enumerate(labels.items()):\n",
        "    if i < 5:\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "# Save as .pt file\n",
        "output_path = \"[path to labels file]\"\n",
        "torch.save(labels, output_path)\n",
        "print(f\"\\nLabels saved to: {output_path}\")\n",
        "\n",
        "# Verify the saved file\n",
        "loaded_labels = torch.load(output_path)\n",
        "print(f\"\\nVerification - loaded {len(loaded_labels)} labels\")\n",
        "print(\"Sample loaded labels:\")\n",
        "for i, (key, value) in enumerate(loaded_labels.items()):\n",
        "    if i < 3:\n",
        "        print(f\"{key}: {value}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_labels = torch.load(output_path)\n",
        "print(f\"\\nVerification - loaded {len(test_labels)} labels\")\n",
        "print(\"Sample loaded labels:\")\n",
        "for i, (key, value) in enumerate(test_labels.items()):\n",
        "    if i > 100:\n",
        "        print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "mesh_dir = \"[path to mesh folder: mesh_minimal]\"  # <-- update if different\n",
        "labels_path = \"[path to mesh folder: mesh_minimal]_scaled_obj_files/labels.pt\"\n",
        "\n",
        "# load labels\n",
        "labels = torch.load(labels_path)\n",
        "label_ids = set(labels.keys())\n",
        "\n",
        "# collect mesh ids\n",
        "mesh_ids = set()\n",
        "for name in os.listdir(mesh_dir):\n",
        "    if name.startswith(\".\"):\n",
        "        continue\n",
        "    stem = os.path.splitext(name)[0]  # remove extension if any\n",
        "    # handle names like 9478504_femur or 9478504_femur.obj\n",
        "    mesh_id = stem.split(\"_femur\")[0]\n",
        "    if mesh_id:\n",
        "        mesh_ids.add(mesh_id)\n",
        "\n",
        "missing_in_labels = sorted(mesh_ids - label_ids)\n",
        "extra_in_labels = sorted(label_ids - mesh_ids)\n",
        "\n",
        "print(f\"Mesh files found: {len(mesh_ids)}\")\n",
        "print(f\"Labels found: {len(label_ids)}\")\n",
        "print(f\"Missing in labels: {len(missing_in_labels)}\")\n",
        "print(f\"Extra in labels: {len(extra_in_labels)}\")\n",
        "\n",
        "if missing_in_labels:\n",
        "    print(\"Sample missing in labels:\", missing_in_labels[:10])\n",
        "if extra_in_labels:\n",
        "    print(\"Sample extra in labels:\", extra_in_labels[:10])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "base_dir = \"[path to mesh folder: mesh_minimal]\"  # update if different\n",
        "diseased_dir = os.path.join(base_dir, \"diseased_tagged\")\n",
        "healthy_dir = os.path.join(base_dir, \"healthy_tagged\")\n",
        "labels_path = \"[path to mesh folder: mesh_minimal]_scaled_obj_files/labels.pt\"\n",
        "\n",
        "def extract_tagged_ids(folder, suffix):\n",
        "    ids = set()\n",
        "    for name in os.listdir(folder):\n",
        "        if name.startswith(\".\"):\n",
        "            continue\n",
        "        stem = os.path.splitext(name)[0]\n",
        "        if not stem.endswith(suffix):\n",
        "            continue\n",
        "        stem = stem[:-len(suffix)]  # remove _less / _more\n",
        "        # handle names like 9478504_femur\n",
        "        if \"_femur\" in stem:\n",
        "            stem = stem.split(\"_femur\")[0]\n",
        "        if stem:\n",
        "            ids.add(stem)\n",
        "    return ids\n",
        "\n",
        "less_ids = extract_tagged_ids(diseased_dir, \"_less\")\n",
        "more_ids = extract_tagged_ids(healthy_dir, \"_more\")\n",
        "tagged_ids = less_ids | more_ids\n",
        "\n",
        "print(f\"Tagged (_less) ids: {len(less_ids)}\")\n",
        "print(f\"Tagged (_more) ids: {len(more_ids)}\")\n",
        "print(f\"Total tagged ids: {len(tagged_ids)}\")\n",
        "\n",
        "labels = torch.load(labels_path)\n",
        "before = len(labels)\n",
        "\n",
        "# remove tagged ids\n",
        "labels = {k: v for k, v in labels.items() if k not in tagged_ids}\n",
        "after = len(labels)\n",
        "\n",
        "torch.save(labels, labels_path)\n",
        "\n",
        "print(f\"Labels before: {before}\")\n",
        "print(f\"Labels after: {after}\")\n",
        "print(f\"Removed from labels: {before - after}\")\n",
        "\n",
        "# tests\n",
        "still_present = [pid for pid in tagged_ids if pid in labels]\n",
        "assert not still_present, f\"Tagged IDs still present in labels: {still_present[:10]}\"\n",
        "print(\"Test OK: no tagged patient IDs remain in labels.pt\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "base_dir = \"[path to mesh folder: mesh_minimal]\"  # update if needed\n",
        "diseased_dir = os.path.join(base_dir, \"diseased_tagged\")\n",
        "healthy_dir = os.path.join(base_dir, \"healthy_tagged\")\n",
        "labels_path = \"[path to labels file]\"\n",
        "\n",
        "def extract_tagged_ids(folder, suffix):\n",
        "    ids = set()\n",
        "    for name in os.listdir(folder):\n",
        "        if name.startswith(\".\"):\n",
        "            continue\n",
        "        stem = os.path.splitext(name)[0]\n",
        "        if not stem.endswith(suffix):\n",
        "            continue\n",
        "        stem = stem[:-len(suffix)]  # remove _less / _more\n",
        "        if \"_femur\" in stem:\n",
        "            stem = stem.split(\"_femur\")[0]\n",
        "        if stem:\n",
        "            ids.add(stem)\n",
        "    return ids\n",
        "\n",
        "# build tagged set\n",
        "less_ids = extract_tagged_ids(diseased_dir, \"_less\")\n",
        "more_ids = extract_tagged_ids(healthy_dir, \"_more\")\n",
        "tagged_ids = less_ids | more_ids\n",
        "\n",
        "print(f\"Tagged (_less) ids: {len(less_ids)}\")\n",
        "print(f\"Tagged (_more) ids: {len(more_ids)}\")\n",
        "print(f\"Total tagged ids: {len(tagged_ids)}\")\n",
        "\n",
        "labels = torch.load(labels_path)\n",
        "label_ids = set(labels.keys())\n",
        "\n",
        "# explicit checks BEFORE removal\n",
        "intersect_before = label_ids & tagged_ids\n",
        "missing_in_labels = tagged_ids - label_ids\n",
        "print(f\"Tagged ids present in labels (before): {len(intersect_before)}\")\n",
        "print(f\"Tagged ids missing in labels (before): {len(missing_in_labels)}\")\n",
        "\n",
        "# remove tagged ids\n",
        "labels = {k: v for k, v in labels.items() if k not in tagged_ids}\n",
        "torch.save(labels, labels_path)\n",
        "\n",
        "# explicit checks AFTER removal\n",
        "label_ids_after = set(labels.keys())\n",
        "intersect_after = label_ids_after & tagged_ids\n",
        "print(f\"Tagged ids present in labels (after): {len(intersect_after)}\")\n",
        "\n",
        "# tests\n",
        "assert len(intersect_after) == 0, f\"Tagged IDs still in labels: {list(intersect_after)[:10]}\"\n",
        "assert len(intersect_before) == (len(label_ids) - len(label_ids_after)), \"Removed count mismatch\"\n",
        "print(\"Tests OK: tagged IDs removed and counts consistent.\")\n",
        "\n",
        "# count healthy/diseased in labels (disease_diagnosis is first element)\n",
        "healthy = 0\n",
        "diseased = 0\n",
        "unknown = 0\n",
        "for v in labels.values():\n",
        "    # v can be numpy array or list/torch tensor\n",
        "    if isinstance(v, torch.Tensor):\n",
        "        val = float(v[0].item())\n",
        "    else:\n",
        "        val = float(v[0])\n",
        "    if math.isnan(val):\n",
        "        unknown += 1\n",
        "    elif val == 0:\n",
        "        healthy += 1\n",
        "    elif val == 1:\n",
        "        diseased += 1\n",
        "    else:\n",
        "        unknown += 1\n",
        "\n",
        "print(f\"Labels total: {len(labels)}\")\n",
        "print(f\"Healthy (0): {healthy}\")\n",
        "print(f\"Diseased (1): {diseased}\")\n",
        "print(f\"Unknown/other: {unknown}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import math\n",
        "\n",
        "base_dir = \"[path to mesh folder: mesh_minimal]\"  # update if needed\n",
        "diseased_dir = os.path.join(base_dir, \"diseased_tagged\")\n",
        "healthy_dir = os.path.join(base_dir, \"healthy_tagged\")\n",
        "labels_path = \"[path to mesh folder: mesh_minimal]_scaled_obj_files/labels.pt\"\n",
        "\n",
        "def extract_ids_from_folder(folder):\n",
        "    ids = set()\n",
        "    for name in os.listdir(folder):\n",
        "        if name.startswith(\".\"):\n",
        "            continue\n",
        "        stem = os.path.splitext(name)[0]\n",
        "        if stem.endswith(\"_less\"):\n",
        "            stem = stem[:-5]\n",
        "        if stem.endswith(\"_more\"):\n",
        "            stem = stem[:-5]\n",
        "        if \"_femur\" in stem:\n",
        "            stem = stem.split(\"_femur\")[0]\n",
        "        if stem:\n",
        "            ids.add(stem)\n",
        "    return ids\n",
        "\n",
        "healthy_folder_ids = extract_ids_from_folder(healthy_dir)\n",
        "diseased_folder_ids = extract_ids_from_folder(diseased_dir)\n",
        "\n",
        "labels = torch.load(labels_path)\n",
        "\n",
        "healthy_ids = set()\n",
        "diseased_ids = set()\n",
        "unknown_ids = set()\n",
        "\n",
        "for k, v in labels.items():\n",
        "    val = float(v[0]) if not isinstance(v, torch.Tensor) else float(v[0].item())\n",
        "    if math.isnan(val):\n",
        "        unknown_ids.add(k)\n",
        "    elif val == 0:\n",
        "        healthy_ids.add(k)\n",
        "    elif val == 1:\n",
        "        diseased_ids.add(k)\n",
        "    else:\n",
        "        unknown_ids.add(k)\n",
        "\n",
        "# Checks\n",
        "missing_healthy = healthy_ids - healthy_folder_ids\n",
        "missing_diseased = diseased_ids - diseased_folder_ids\n",
        "wrong_healthy_side = healthy_ids & diseased_folder_ids\n",
        "wrong_diseased_side = diseased_ids & healthy_folder_ids\n",
        "\n",
        "print(f\"Labels total: {len(labels)}\")\n",
        "print(f\"Healthy labels: {len(healthy_ids)}\")\n",
        "print(f\"Diseased labels: {len(diseased_ids)}\")\n",
        "print(f\"Unknown labels: {len(unknown_ids)}\")\n",
        "\n",
        "print(f\"Healthy IDs missing in healthy_tagged: {len(missing_healthy)}\")\n",
        "print(f\"Diseased IDs missing in diseased_tagged: {len(missing_diseased)}\")\n",
        "print(f\"Healthy IDs found in diseased_tagged (wrong side): {len(wrong_healthy_side)}\")\n",
        "print(f\"Diseased IDs found in healthy_tagged (wrong side): {len(wrong_diseased_side)}\")\n",
        "\n",
        "if missing_healthy:\n",
        "    print(\"Sample missing healthy:\", sorted(list(missing_healthy))[:10])\n",
        "if missing_diseased:\n",
        "    print(\"Sample missing diseased:\", sorted(list(missing_diseased))[:10])\n",
        "if wrong_healthy_side:\n",
        "    print(\"Sample wrong-side healthy:\", sorted(list(wrong_healthy_side))[:10])\n",
        "if wrong_diseased_side:\n",
        "    print(\"Sample wrong-side diseased:\", sorted(list(wrong_diseased_side))[:10])\n",
        "\n",
        "# strict tests (uncomment if you want hard failure)\n",
        "# assert not missing_healthy, \"Some healthy label IDs are not in healthy_tagged\"\n",
        "# assert not missing_diseased, \"Some diseased label IDs are not in diseased_tagged\"\n",
        "# assert not wrong_healthy_side, \"Some healthy IDs appear in diseased_tagged\"\n",
        "# assert not wrong_diseased_side, \"Some diseased IDs appear in healthy_tagged\"\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "inr_sdf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}